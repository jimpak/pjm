{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93  88  93]\n",
      " [ 89  91  90]\n",
      " [ 96  98 100]\n",
      " [ 73  66  70]\n",
      " [ 53  46  55]\n",
      " [ 69  74  77]\n",
      " [ 47  56  60]\n",
      " [ 87  79  90]\n",
      " [ 79  70  88]\n",
      " [ 69  70  73]\n",
      " [ 70  65  74]\n",
      " [ 93  95  91]\n",
      " [ 79  80  73]\n",
      " [ 70  73  78]\n",
      " [ 93  89  96]\n",
      " [ 78  75  68]\n",
      " [ 81  90  93]\n",
      " [ 88  92  86]\n",
      " [ 78  83  77]\n",
      " [ 82  86  90]\n",
      " [ 86  82  89]\n",
      " [ 78  83  85]\n",
      " [ 76  83  71]\n",
      " [ 96  93  95]]\n",
      "[[ 93]\n",
      " [ 88]\n",
      " [ 93]\n",
      " [ 89]\n",
      " [ 91]\n",
      " [ 90]\n",
      " [ 96]\n",
      " [ 98]\n",
      " [100]\n",
      " [ 73]\n",
      " [ 66]\n",
      " [ 70]\n",
      " [ 53]\n",
      " [ 46]\n",
      " [ 55]\n",
      " [ 69]\n",
      " [ 74]\n",
      " [ 77]\n",
      " [ 47]\n",
      " [ 56]\n",
      " [ 60]\n",
      " [ 87]\n",
      " [ 79]\n",
      " [ 90]\n",
      " [ 79]\n",
      " [ 70]\n",
      " [ 88]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 73]\n",
      " [ 70]\n",
      " [ 65]\n",
      " [ 74]\n",
      " [ 93]\n",
      " [ 95]\n",
      " [ 91]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 73]\n",
      " [ 70]\n",
      " [ 73]\n",
      " [ 78]\n",
      " [ 93]\n",
      " [ 89]\n",
      " [ 96]\n",
      " [ 78]\n",
      " [ 75]\n",
      " [ 68]\n",
      " [ 81]\n",
      " [ 90]\n",
      " [ 93]\n",
      " [ 88]\n",
      " [ 92]\n",
      " [ 86]\n",
      " [ 78]\n",
      " [ 83]\n",
      " [ 77]\n",
      " [ 82]\n",
      " [ 86]\n",
      " [ 90]\n",
      " [ 86]\n",
      " [ 82]\n",
      " [ 89]\n",
      " [ 78]\n",
      " [ 83]\n",
      " [ 85]\n",
      " [ 76]\n",
      " [ 83]\n",
      " [ 71]\n",
      " [ 96]\n",
      " [ 93]\n",
      " [ 95]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('data/data-01-test-score.csv')\n",
    "data=np.array(df.values)\n",
    "x_data = data[:, :3].reshape(-1,3)\n",
    "t_data = data[:, :-1].reshape(-1,1)\n",
    "print(x_data)\n",
    "print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62766657]\n",
      " [0.07668932]]\n",
      "[0.80977145]\n"
     ]
    }
   ],
   "source": [
    "w=np.random.rand(2,1)\n",
    "b=np.random.rand(1)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimize \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD([w,b], lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      2\u001b[0m optimize\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\optim\\sgd.py:27\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_param_group(cast(\u001b[38;5;28mdict\u001b[39m, param_group))\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\it\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:883\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer can only optimize Tensors, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut one of the params is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtypename(param))\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_leaf \u001b[38;5;129;01mor\u001b[39;00m param\u001b[38;5;241m.\u001b[39mretains_grad):\n\u001b[0;32m    886\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt optimize a non-leaf Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is ndarray"
     ]
    }
   ],
   "source": [
    "optimize = optim.SGD([w,b], lr = 0.01)\n",
    "optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (24,3) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2001\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 3\u001b[0m     y\u001b[38;5;241m=\u001b[39mx_data\u001b[38;5;241m*\u001b[39mw\u001b[38;5;241m+\u001b[39mb\n\u001b[0;32m      4\u001b[0m     cost\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean((t_data\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m     optimize\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (24,3) (2,1) "
     ]
    }
   ],
   "source": [
    "epochs=2001\n",
    "for epoch in range(epochs):\n",
    "    y=x_data*w+b\n",
    "    cost=torch.mean((t_data-y)**2)\n",
    "    optimize.zero_grad()\n",
    "    cost.backward()\n",
    "    optimize.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
